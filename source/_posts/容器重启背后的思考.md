---
layout:    post
title:     容器重启背后的思考
category:  问题排查
description: docker容器重启背后的思考
tags:
- 问题排查
date: 2022/04/14 11:27:10

---
前几天同事喊我说一个容器过一段时间就会重启，然后重启的原因是OOMKilled。第一反应是代码有问题，检查了下最近的提交，并没有发现可疑的代码。看了下jvm参数配置，发现并没有如下配置:
```
-XX:+HeapDumpOnOutOfMemoryError
-XX:HeapDumpPath=/root/logs/java_heapdump.hprof
```
也没有办法证明是JVM发生OOM导致容器被重启。看了下Cat监控的GC情况，发现并没有出现GC以后，内存还在继续增长的情况，说明不太可能是因为JVM发生OOM导致的。然后加了上面的参数，等待复现。
没多久就看到容器重启告警了，检查GC情况发现内存还是可以回收掉的，然后检查日志，日志中也没有出现OutOfMemoryError的错误，/root/logs目录下也并没有生成dump文件。那么问题明朗了，发生OOM的应该不是JVM。

检查下JVM完整的启动参数，参数如下:
```
-javaagent:/root/org.jacoco.agent-5.9.9.jar=output=tcpserver,address=0.0.0.0,port=6300 
## 堆内存最大大小
-Xmx2g 
## 堆内存初始大小
-Xms2g 
## 年轻代大小(eden+ 2 survivor)
-Xmn1g 
## 元空间最大大小
-XX:MaxMetaspaceSize=1g 
## 元空间大小
-XX:MetaspaceSize=256m
## 最大DirectMemory
-XX:MaxDirectMemorySize=96m
## 每个线程的栈大小
-Xss256k
## STW 工作线程数
-XX:ParallelGCThreads=2 
-DignoreCaptchaCheck=true 
-DmockLuckBag=true 
## 不省略异常(NullPointerException、ArithmeticException、ArrayIndexOutOfBoundsException、ArrayStoreException、ClassCastException)
-XX:-OmitStackTraceInFastThrow 
## Does nothing. Provided for backward compatibility.
-Xdebug 
## 远程调试参数
-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1088
```
### 凶手是Docker？
Docker的配置为2C 2G。会不会是Docker容器把我们的进程杀死了呢？因为我们设置堆的最大内存是2G，而整个Docker容器也仅仅只有2G的内存空间。对于Linux系统来说，一旦内核检测到没有足够的内存可以分配，就会扔出OOM，并开始杀死一些进程用于释放内存空间，但是糟糕的是任何进程都可能成为内核猎杀的对象，包括docker daemon和其它一些重要的进程。更危险的是如果某个支撑系统运行的重要进程被干掉了，整个系统也就宕掉了。如果大量的容器把主机的内存消耗殆尽，OOM被触发后系统内核立即开始杀进程释放内存。如果内核杀死的第一个进程就是docker daemon会怎么样？结果就是没办法管理运行中的容器，这是不可以接受的。

针对这个问题，docker尝试通过调整docker daemon的OOM优先级来进行缓解。内核在选择要杀死的进程时会对所有的进程打分，直接杀死的分最高的进程，接着是下一个。当docker daemon的OOM优先级被降低后（注意容器进程的OOM优先级并没有被调整），docker daemon进程的得分不仅会低于容器进程的得分，还会低于其它一些进程的的分，这样docker daemon进程就安全多了。

不过docker的官方文档中一直强调这只是一种缓解的方案，并且为我们提供了一些降低风险的建议：

- 通过测试掌握应用对内存的需求
- 保证运行容器的主机有充足的内存
- 限制容器可以使用的内存
- 为主机配置swap
  
其实说白了，就是通过限制容器使用的内存上限，来降低主机内存耗尽时带来的各种风险。

### Docker相关知识
Docker使用Google公司推出的Go语言进行开发实现，基于Linux内核的cgroup，namespace，以及OverlayFS类的Union FS等技术，对进程进行封装隔离，属于操作系统层面的虚拟化技术。

cgroup，名称源自控制组群（control groups）的简写，是Linux内核的一个功能，用来限制、控制与分离一个进程组的资源（如CPU、内存、磁盘输入输出等）。cgroups的一个设计目标是为不同的应用情况提供统一的接口，从控制单一进程到操作系统层虚拟化。cgroups提供：

- 资源限制：组可以被设置不超过设定的内存限制；也包括虚拟内存
- 优先级：一些组可能会得到大量的CPU或磁盘IO吞吐量
- 结算：用来度量系统实际用了多少资源
- 控制：冻结组或检查点和重启动

#### 更高效的利用系统资源
Docker容器因为不需要硬件虚拟以及完整操作系统等额外开销，Docker对系统资源的利用率更高。无论是应用执行速度、内存损耗或者文件存储速度，都要比传统虚拟机技术更高效。因此，相比虚拟机技术，一个相同配置的主机，往往可以运行更多数量的应用。

#### 更快速的启动时间
传统的虚拟机技术启动应用服务往往需要数分钟，而Docker容器应用，由于直接运行于宿主内核，无需启动完整的操作系统，因此可以做到秒级、甚至毫秒级的启动时间，大大节约了时间。

#### 一致的运行环境
开发过程中一个常见的问题是环境一致性问题。由于开发环境、测试环境、生产环境不一致，导致有些bug并未在开发过程中被发现。而Docker的镜像提供了除内核外完整的运行时环境，确保了应用运行环境的一致。

#### 持续交付和部署
我们希望一次创建或者配置，可以在任意地方正常运行。使用Docker可以通过定制应用镜像来实现持续集成、持续交付、部署。开发人员可以通过Dockerfile来进行镜像构建，并结合持续集成系统进行集成测试，而运维人员可以直接在生产环境中快速部署该镜像，甚至结合持续部署系统进行自动化部署。

使用Dockerfile使镜像构建透明化，不仅开发团队可以理解应用运行环境，也方便运维团队理解应用运行所需条件，帮助更好的生产环境中部署该镜像。

#### 更轻松的迁移
由于Docker确保了执行环境的一致性，使得应用的迁移更加容易。Docker可以在很多平台上运行，无论是物理机、虚拟机、公有云、私有云，运行结果都是一致的。用户可以很轻松的将在一个平台上运行的应用，迁移到另一个平台上，而不用担心运行环境的变化导致应用无法正常运行的情况。

#### 更轻松的维护和扩展
Docker使用的分层存储以及镜像的技术，使得应用重复部分的服用更为容易，也使得应用的维护更加更加简单，基于基础镜像进一步扩展镜像也变得非常简单。Docker团队同各个开源项目团队一起维护了一大批高质量的官方镜像，既可以直接在生产环境使用，又可以作为基础进一步定制，大大的降低了应用服务的镜像制作成本。

#### Docker容器对比虚拟机总结
| 特性       | 容器               | 虚拟机     |
| ---------- | ------------------ | ---------- |
| 启动       | 秒级               | 分钟级     |
| 硬盘使用   | 一般为MB           | 一般为GB   |
| 性能       | 接近原生           | 弱于       |
| 系统支持量 | 单机支持上千个容器 | 一般几十个 |

#### JVM对资源的限制
在不显式的指明一些参数的时候，JVM通常会读取一些数据作为默认值。那么JVM运行在Docker容器中，读取的内存，cpu数是用的物理机的还是Docker限制的cgroup的呢？

#### CPU
如果不显式的指定 *-XX:ParallelGCThreads* and *-XX:CICompilerCount*，那么JVM就会根据读到的CPU数目进行计算来设置数值。

在计算Parallel GC的Threads数目的地方`runtime\vm_version.cpp`  （基于1.8.0_121-b36）

```c++
if (FLAG_IS_DEFAULT(ParallelGCThreads)) {
    assert(ParallelGCThreads == 0, "Default ParallelGCThreads is not 0");
    // For very large machines, there are diminishing returns
    // for large numbers of worker threads.  Instead of
    // hogging the whole system, use a fraction of the workers for every
    // processor after the first 8.  For example, on a 72 cpu machine
    // and a chosen fraction of 5/8
    // use 8 + (72 - 8) * (5/8) == 48 worker threads.
    unsigned int ncpus = (unsigned int) os::initial_active_processor_count();
    return (ncpus <= switch_pt) ?
           ncpus :
          (switch_pt + ((ncpus - switch_pt) * num) / den);
  } else {
    return ParallelGCThreads;
  }
```
调用获取cpu数目的os::initial_active_processor_count()方法：
```c++
int os::active_processor_count() {
  cpu_set_t cpus;  // can represent at most 1024 (CPU_SETSIZE) processors
  int cpus_size = sizeof(cpu_set_t);
  int cpu_count = 0;

  // pid 0 means the current thread - which we have to assume represents the process
  if (sched_getaffinity(0, cpus_size, &cpus) == 0) {
    cpu_count = os_cpu_count(&cpus);
    if (PrintActiveCpus) {
      tty->print_cr("active_processor_count: sched_getaffinity processor count: %d", cpu_count);
    }
  }
  else {
    cpu_count = ::sysconf(_SC_NPROCESSORS_ONLN);
    warning("sched_getaffinity failed (%s)- using online processor count (%d) "
            "which may exceed available processors", strerror(errno), cpu_count);
  }

  assert(cpu_count > 0 && cpu_count <= processor_count(), "sanity check");
  return cpu_count;
}
```

可以看到上面的代码，其中有一个`sched_getaffinity()`获取cpu亲和力这个方法，这个方法看解释是获取进程的掩码写入到`cpu_set_t`中，进程的Cpu关联掩码决定了它有资格运行的Cpu集。成功返回0，错误返回-1或错误码。具体可以参考这个 https://linux.die.net/man/2/sched_getaffinity 。

```c++
static int os_cpu_count(const cpu_set_t* cpus) {
  int count = 0;
  // only look up to the number of configured processors
  for (int i = 0; i < os::processor_count(); i++) {
    if (CPU_ISSET(i, cpus)) {
      count++;
    }
  }
  return count;
}
```
这里调用processor_count函数来获取所有的CPU核数。然后去检查是否在这个cpu是不是集合的成员，是成员则增加计数。

_processor_count这个变量通过set_processor_count函数去赋值。

```c++
void os::Linux::initialize_system_info() {
  set_processor_count(sysconf(_SC_NPROCESSORS_CONF));
  if (processor_count() == 1) {
    pid_t pid = os::Linux::gettid();
    char fname[32];
    jio_snprintf(fname, sizeof(fname), "/proc/%d", pid);
    FILE *fp = fopen(fname, "r");
    if (fp == NULL) {
      unsafe_chroot_detected = true;
    } else {
      fclose(fp);
    }
  }
  _physical_memory = (julong)sysconf(_SC_PHYS_PAGES) * (julong)sysconf(_SC_PAGESIZE);
  assert(processor_count() > 0, "linux error");
}
```

其实获取CPU核数都是通过调用系统sysconf函数来获取，只不过一个是`_SC_NPROCESSORS_CONF` 一个是 `_SC_NPROCESSORS_ONLN` 。

**_SC_NPROCESSORS_CONF** ：返回系统所有的CPU核数，这个值也包括系统中禁止用户使用的CPU个数；

**_SC_NPROCESSORS_ONLN**：返回系统中可用的CPU核数；

#### 内存
既然读取CPU会出错，那么内存应该也是一样的，在不显式的指定一些参数时如-Xmx(MaxHeapSize)、-Xms(InitialHeapSize)时，JVM会根据它读取到的机器的内存大小做一些默认的设置如：

```c++
void Arguments::set_heap_size() {
  if (!FLAG_IS_DEFAULT(DefaultMaxRAMFraction)) {
    // Deprecated flag
    FLAG_SET_CMDLINE(uintx, MaxRAMFraction, DefaultMaxRAMFraction);
  }
  
  julong phys_mem =
    FLAG_IS_DEFAULT(MaxRAM) ? MIN2(os::physical_memory(), (julong)MaxRAM)
                            : (julong)MaxRAM;
  
  // Experimental support for CGroup memory limits
  if (UseCGroupMemoryLimitForHeap) {
    // This is a rough indicator that a CGroup limit may be in force
    // for this process
    const char* lim_file = "/sys/fs/cgroup/memory/memory.limit_in_bytes";
    FILE *fp = fopen(lim_file, "r");
    if (fp != NULL) {
      julong cgroup_max = 0;
      int ret = fscanf(fp, JULONG_FORMAT, &cgroup_max);
      if (ret == 1 && cgroup_max > 0) {
        // If unlimited, cgroup_max will be a very large, but unspecified
        // value, so use initial phys_mem as a limit
        if (PrintGCDetails && Verbose) {
          // Cannot use gclog_or_tty yet.
          tty->print_cr("Setting phys_mem to the min of cgroup limit ("
                        JULONG_FORMAT "MB) and initial phys_mem ("
                        JULONG_FORMAT "MB)", cgroup_max/M, phys_mem/M);
        }
        phys_mem = MIN2(cgroup_max, phys_mem);
      } else {
        warning("Unable to read/parse cgroup memory limit from %s: %s",
                lim_file, errno != 0 ? strerror(errno) : "unknown error");
      }
      fclose(fp);
    } else {
      warning("Unable to open cgroup memory limit file %s (%s)", lim_file, strerror(errno));
    }
  }
```
意外发现竟然有对CGroup的支持，只不过是实验支持，可以使用-XX:+UseCGroupMemoryLimitForHeap开启这个支持，不过既然是实验支持可能还不太支持，最好还是手动指定一下比较稳妥。

在 [JDK-8146115]("https://bugs.openjdk.java.net/browse/JDK-8146115") 中发现Docker的增强已经在JDK10中实现了，使用-XX:+UseContainerSupport可以开启容器支持，这一增强已经被Backports 到了JDK8的一些新版本中。

最新版的os::active_processor_count()变成了：
```c++
// Determine the active processor count from one of
// three different sources:
//
// 1. User option -XX:ActiveProcessorCount
// 2. kernel os calls (sched_getaffinity or sysconf(_SC_NPROCESSORS_ONLN)
// 3. extracted from cgroup cpu subsystem (shares and quotas)
//
// Option 1, if specified, will always override.
// If the cgroup subsystem is active and configured, we
// will return the min of the cgroup and option 2 results.
// This is required since tools, such as numactl, that
// alter cpu affinity do not update cgroup subsystem
// cpuset configuration files.
int os::active_processor_count() {
  // User has overridden the number of active processors
  if (ActiveProcessorCount > 0) {
    if (PrintActiveCpus) {
      tty->print_cr("active_processor_count: "
                    "active processor count set by user : %d",
                    ActiveProcessorCount);
    }
    return ActiveProcessorCount;
  }

  int active_cpus;
  if (OSContainer::is_containerized()) {
    active_cpus = OSContainer::active_processor_count();
    if (PrintActiveCpus) {
      tty->print_cr("active_processor_count: determined by OSContainer: %d",
                     active_cpus);
    }
  } else {
    active_cpus = os::Linux::active_processor_count();
  }

  return active_cpus;
}
```
可以看到如果有-XX:ActiveProcessorCount参数则使用参数，如果没有就会去OSContainer::is_containerized()判断是否是容器，然后获取容器对资源的限制。

```c++
inline bool OSContainer::is_containerized() {
  assert(_is_initialized, "OSContainer not initialized");
  return _is_containerized;
}
```

这里_is_containerized是由Threads::create_vm调用OSContainer::init()时检查虚拟机是否运行在容器中得来的。（osContainer_linux.hpp）

```c++
/* init
 *
 * Initialize the container support and determine if
 * we are running under cgroup control.
 */
void OSContainer::init() {
  FILE *mntinfo = NULL;
  FILE *cgroup = NULL;
  char buf[MAXPATHLEN+1];
  char tmproot[MAXPATHLEN+1];
  char tmpmount[MAXPATHLEN+1];
  char *p;
  jlong mem_limit;

  assert(!_is_initialized, "Initializing OSContainer more than once");

  _is_initialized = true;
  _is_containerized = false;

  _unlimited_memory = (LONG_MAX / os::vm_page_size()) * os::vm_page_size();

  if (PrintContainerInfo) {
    tty->print_cr("OSContainer::init: Initializing Container Support");
  }
  if (!UseContainerSupport) {
    if (PrintContainerInfo) {
      tty->print_cr("Container Support not enabled");
    }
    return;
  }
  
  /*
   * Find the cgroup mount point for memory and cpuset
   * by reading /proc/self/mountinfo
   *
   * Example for docker:
   * 219 214 0:29 /docker/7208cebd00fa5f2e342b1094f7bed87fa25661471a4637118e65f1c995be8a34 /sys/fs/cgroup/memory ro,nosuid,nodev,noexec,relatime - cgroup cgroup rw,memory
   *
   * Example for host:
   * 34 28 0:29 / /sys/fs/cgroup/memory rw,nosuid,nodev,noexec,relatime shared:16 - cgroup cgroup rw,memory
   */
   mntinfo = fopen("/proc/self/mountinfo", "r");
  if (mntinfo == NULL) {
      if (PrintContainerInfo) {
        tty->print_cr("Can't open /proc/self/mountinfo, %s",
                       strerror(errno));
      }
      return;
  }
  ···
    /*
   * Read /proc/self/cgroup and map host mount point to
   * local one via /proc/self/mountinfo content above
   *
   * Docker example:
   * 5:memory:/docker/6558aed8fc662b194323ceab5b964f69cf36b3e8af877a14b80256e93aecb044
   *
   * Host example:
   * 5:memory:/user.slice
   *
   * Construct a path to the process specific memory and cpuset
   * cgroup directory.
   *
   * For a container running under Docker from memory example above
   * the paths would be:
   *
   * /sys/fs/cgroup/memory
   *
   * For a Host from memory example above the path would be:
   *
   * /sys/fs/cgroup/memory/user.slice
   *
   */
    cgroup = fopen("/proc/self/cgroup", "r");
  if (cgroup == NULL) {
    if (PrintContainerInfo) {
      tty->print_cr("Can't open /proc/self/cgroup, %s",
                     strerror(errno));
      }
    return;
  }
  ···
  _is_containerized = true;
}
```
可以看到这里初始化的时候会先判断-XX:+UseContainerSupport 是否开启，/proc/self/mountinfo 、/proc/self/cgroup 是否可读等，如果运行在容器中，那么就会调用OSContainer::active_processor_count获取容器限制的CPU数目（osContainer_linux.hpp）：

```c++
/* active_processor_count
 *
 * Calculate an appropriate number of active processors for the
 * VM to use based on these three inputs.
 *
 * cpu affinity
 * cgroup cpu quota & cpu period
 * cgroup cpu shares
 *
 * Algorithm:
 *
 * Determine the number of available CPUs from sched_getaffinity
 *
 * If user specified a quota (quota != -1), calculate the number of
 * required CPUs by dividing quota by period.
 *
 * If shares are in effect (shares != -1), calculate the number
 * of CPUs required for the shares by dividing the share value
 * by PER_CPU_SHARES.
 *
 * All results of division are rounded up to the next whole number.
 *
 * If neither shares or quotas have been specified, return the
 * number of active processors in the system.
 *
 * If both shares and quotas have been specified, the results are
 * based on the flag PreferContainerQuotaForCPUCount.  If true,
 * return the quota value.  If false return the smallest value
 * between shares or quotas.
 *
 * If shares and/or quotas have been specified, the resulting number
 * returned will never exceed the number of active processors.
 *
 * return:
 *    number of CPUs
 */
int OSContainer::active_processor_count() {
  int quota_count = 0, share_count = 0;
  int cpu_count, limit_count;
  int result;

  // We use a cache with a timeout to avoid performing expensive
  // computations in the event this function is called frequently.
  // [See 8227006].
  if (!cpu->cache_has_expired()) {
    if (PrintContainerInfo) {
      tty->print_cr("OSContainer::active_processor_count (cached): %d", OSContainer::_active_processor_count);
    }

    return OSContainer::_active_processor_count;
  }

  cpu_count = limit_count = os::Linux::active_processor_count();
  int quota  = cpu_quota();
  int period = cpu_period();
  int share  = cpu_shares();

  if (quota > -1 && period > 0) {
    quota_count = ceilf((float)quota / (float)period);
    if (PrintContainerInfo) {
      tty->print_cr("CPU Quota count based on quota/period: %d", quota_count);
    }
  }
  if (share > -1) {
    share_count = ceilf((float)share / (float)PER_CPU_SHARES);
    if (PrintContainerInfo) {
      tty->print_cr("CPU Share count based on shares: %d", share_count);
    }
  }

  // If both shares and quotas are setup results depend
  // on flag PreferContainerQuotaForCPUCount.
  // If true, limit CPU count to quota
  // If false, use minimum of shares and quotas
  if (quota_count !=0 && share_count != 0) {
    if (PreferContainerQuotaForCPUCount) {
      limit_count = quota_count;
    } else {
      limit_count = MIN2(quota_count, share_count);
    }
  } else if (quota_count != 0) {
    limit_count = quota_count;
  } else if (share_count != 0) {
    limit_count = share_count;
  }

  result = MIN2(cpu_count, limit_count);
  if (PrintContainerInfo) {
    tty->print_cr("OSContainer::active_processor_count: %d", result);
  }

  // Update the value and reset the cache timeout
  OSContainer::_active_processor_count = result;
  cpu->set_cache_expiry_time(OSCONTAINER_CACHE_TIMEOUT);

  return result;
}
```
可以发现最终的结果和很多变量有关，如果同时设置了共享和配额，则设置结果取决于在首选容器QuotaForcpUCount标志上。

如果为true，则将CPU计数限制为配额，如果为false，则使用最低份额和配额。Docker可以通过-cpu-period、-cpu-quato来进行相关参数的设置，具体可以参考 [Docker文档](https://docs.docker.com/config/containers/resource_constraints/#cpu) 。

最终结果还会和物理机的cpu核数比较来获取较小的值。

内存的获取并没有什么区别，思路基本上与获取cpu的一致。

#### 总结
使用JVM参数取来限制JVM使用容器的资源，可以更好的保护JVM的运行，不会因为超过容器的限制而被Kill掉，容器Kill掉超过限制的进程也是为了保护容器不会被操作系统Kill。



